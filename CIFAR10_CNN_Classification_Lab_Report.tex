\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{multirow}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{fancyvrb}
\usepackage{color}
\geometry{margin=1in}
\pagenumbering{gobble}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

% -------------------- FRONT PAGE --------------------
\begin{center}
    \includegraphics[width=5cm]{gub_logo.png}\\[3em]
    \textbf{\Large Green University of Bangladesh}\\[0.5em]
    \textbf{Department of Computer Science and Engineering (CSE)}\\[0.5em]
    Faculty of Sciences and Engineering\\
    Semester: \textbf{Summer}, Year: \textbf{2025}, B.Sc. in CSE\\[1.5em]
    \textbf{\Large LAB REPORT NO 5.1}\\[1em]
    \textbf{Course Title:} Machine Learning Lab\\
    \textbf{Course Code:} CSE 412 \hspace{1cm} \textbf{Section:} 221\ D6\\[2em]
    \textbf{Lab Report Name: Convolutional Neural Network for CIFAR-10 Image Classification}\\[2em]
    \textbf{\underline{Student Details}}\\[0.5em]
    \begin{tabular}{|>{\centering\arraybackslash}m{6cm}|>{\centering\arraybackslash}m{6cm}|}
        \hline
        \textbf{Name} & \textbf{ID} \\
        \hline
        Md Shihab Uddin Munsi & 212002054 \\
        \hline
    \end{tabular}\\[2em]
    \begin{tabular}{ll}
        \textbf{Lab Date:} & 19/07/2025\\
        \textbf{Submission Date:} & 25/07/2025 \\
        \textbf{Course Teacher's Name:} & Md Naimul Pathan \\
    \end{tabular}\\[2em]
    \fcolorbox{black}{gray!10}{
        \begin{minipage}{0.95\textwidth}
            \centering
            \textbf{\textcolor{blue}{[For Teachers use only:}} \textcolor{red}{Don't Write Anything inside this box}\textbf{\textcolor{blue}{]}}\\[1em]
            \textbf{Lab Report Status}\\[0.5em]
            \begin{tabular}{p{0.2\textwidth} p{0.3\textwidth} p{0.2\textwidth} p{0.3\textwidth}}
                Marks: \dotfill & & Signature: \dotfill & \\
                Comments: \dotfill & & Date: \dotfill & \\
            \end{tabular}
        \end{minipage}
    }
\end{center}

\newpage
\pagenumbering{arabic}

% -------------------- LAB REPORT CONTENT --------------------

\section{Title of the Lab Implementation}
Convolutional Neural Network (CNN) for CIFAR-10 Image Classification

\section{Introduction}
In the field of computer vision, image classification represents one of the fundamental tasks where algorithms are designed to categorize images into predefined classes. The CIFAR-10 dataset is a well-known benchmark dataset consisting of 60,000 32x32 color images across 10 different classes, making it an ideal candidate for developing and evaluating image classification algorithms.

This lab focuses on implementing a Convolutional Neural Network (CNN) using TensorFlow and Keras to classify images from the CIFAR-10 dataset. CNNs have proven to be highly effective for image-related tasks due to their ability to automatically learn spatial hierarchies of features from input images. Unlike traditional machine learning approaches that require manual feature extraction, CNNs can learn these features directly from the raw pixel data.

The implementation follows a structured approach beginning with data exploration and preprocessing, followed by model design, training, evaluation, and experimentation with different architectural modifications to improve performance. This comprehensive approach allows for a deeper understanding of both the dataset characteristics and the impact of various CNN architectural choices on classification accuracy.

\section{Objectives}
\begin{itemize}
    \item To load and explore the CIFAR-10 dataset, understanding its structure and distribution
    \item To preprocess image data appropriately for neural network training
    \item To design and implement a CNN architecture using TensorFlow/Keras
    \item To train the CNN model and track its performance metrics
    \item To evaluate the trained model on test data using accuracy, confusion matrix, and classification reports
    \item To experiment with architecture modifications such as Dropout and Batch Normalization
    \item To compare and analyze the performance differences between baseline and improved models
    \item To visualize model predictions and identify patterns in correct and incorrect classifications
\end{itemize}

\section{Tools \& Technologies Used}
\begin{itemize}
    \item \textbf{Python 3.10:} Primary programming language
    \item \textbf{TensorFlow 2.19.0:} Deep learning framework for building and training neural networks
    \item \textbf{Keras:} High-level neural networks API running on top of TensorFlow
    \item \textbf{NumPy:} Library for numerical computations and array operations
    \item \textbf{Matplotlib:} Visualization library for creating plots and charts
    \item \textbf{Seaborn:} Statistical data visualization based on Matplotlib
    \item \textbf{Scikit-learn:} Machine learning library for evaluation metrics and data splitting
    \item \textbf{Jupyter Notebook:} Interactive development environment for code execution and documentation
\end{itemize}

\section{Implementation Details}

\subsection{Dataset Description}
The CIFAR-10 dataset consists of 60,000 32x32 color images distributed evenly across 10 classes:
\begin{itemize}
    \item Airplane
    \item Automobile
    \item Bird
    \item Cat
    \item Deer
    \item Dog
    \item Frog
    \item Horse
    \item Ship
    \item Truck
\end{itemize}

The dataset is divided into 50,000 training images and 10,000 test images. Each image is a 32x32 pixel RGB image (3 color channels), represented as a 32x32x3 array with pixel values ranging from 0 to 255.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{class_distribution.png}
    \caption{Class Distribution in CIFAR-10 Training Set}
\end{figure}

\subsection{Data Importing and Exploration}
The implementation begins with importing the necessary libraries and loading the CIFAR-10 dataset using Keras' built-in dataset utilities:

\begin{lstlisting}[language=Python, caption=Loading and Exploring the CIFAR-10 Dataset]
# Load CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Print the shapes of the dataset
print("Training data shape:", X_train.shape)
print("Training labels shape:", y_train.shape)
print("Test data shape:", X_test.shape)
print("Test labels shape:", y_test.shape)

# Class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']
\end{lstlisting}

The exploration revealed:
\begin{itemize}
    \item Training data consists of 50,000 images of shape (32, 32, 3)
    \item Test data consists of 10,000 images of shape (32, 32, 3)
    \item Each class has exactly 5,000 training images, ensuring a balanced dataset
    \item Labels are single integers representing the class index (0-9)
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{sample_images.png}
    \caption{Sample Images from Each Class in the CIFAR-10 Dataset}
\end{figure}

The visual inspection of sample images revealed the diversity within each class and the challenging nature of the classification task due to variations in object pose, lighting conditions, and background.

\subsection{Data Preprocessing}
To prepare the data for neural network training, several preprocessing steps were implemented:

\begin{lstlisting}[language=Python, caption=Data Preprocessing]
# Normalize pixel values to range [0, 1]
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Convert class labels to one-hot encoded vectors
num_classes = 10
y_train_one_hot = to_categorical(y_train, num_classes)
y_test_one_hot = to_categorical(y_test, num_classes)

# Split training data into training and validation sets
X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(
    X_train, y_train_one_hot, test_size=0.2, random_state=42
)
\end{lstlisting}

The preprocessing steps included:
\begin{enumerate}
    \item \textbf{Normalization:} Scaling pixel values from the original range [0, 255] to [0, 1] to improve training stability and convergence
    \item \textbf{One-hot encoding:} Converting integer labels to one-hot encoded vectors (e.g., class 3 becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
    \item \textbf{Train-validation split:} Dividing the training data into 80\% for training and 20\% for validation to monitor model performance during training
\end{enumerate}

\subsection{CNN Architecture Design}
Two CNN architectures were designed and implemented:

\subsubsection{Baseline CNN Model}
The baseline model consisted of a sequential stack of convolutional and pooling layers followed by fully connected layers:

\begin{lstlisting}[language=Python, caption=Baseline CNN Architecture]
# Build the baseline CNN model
def create_baseline_model():
    model = Sequential([
        # First Convolutional Block
        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
        Conv2D(32, (3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        
        # Second Convolutional Block
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        
        # Flatten and Dense layers
        Flatten(),
        Dense(512, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    
    return model
\end{lstlisting}

The baseline architecture included:
\begin{itemize}
    \item Two convolutional blocks, each with two Conv2D layers and a MaxPooling layer
    \item 32 filters in the first block and 64 filters in the second block
    \item ReLU activation functions throughout the network
    \item A flattening operation to convert 3D feature maps to 1D feature vectors
    \item A fully connected layer with 512 neurons
    \item Output layer with 10 neurons (one for each class) and softmax activation
\end{itemize}

\subsubsection{Improved CNN Model}
The improved model built upon the baseline with additional layers and regularization techniques:

\begin{lstlisting}[language=Python, caption=Improved CNN Architecture]
# Create an improved CNN model with Dropout and Batch Normalization
def create_improved_model():
    model = Sequential([
        # First Convolutional Block with Batch Normalization
        Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)),
        BatchNormalization(),
        Conv2D(32, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.2),
        
        # Second Convolutional Block
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(64, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.3),
        
        # Third Convolutional Block
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        Conv2D(128, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.4),
        
        # Flatten and Dense layers
        Flatten(),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    return model
\end{lstlisting}

The improved model featured:
\begin{itemize}
    \item Three convolutional blocks with increasing filter counts (32, 64, 128)
    \item Batch Normalization after each convolutional layer
    \item Dropout layers with increasing rates (0.2, 0.3, 0.4, 0.5)
    \item A deeper architecture with more capacity to learn complex patterns
\end{itemize}

\subsection{Data Augmentation}
To improve model generalization and robustness, data augmentation was implemented for the improved model:

\begin{lstlisting}[language=Python, caption=Data Augmentation Implementation]
# Create data augmentation
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])
\end{lstlisting}

The augmentation pipeline included:
\begin{itemize}
    \item Horizontal flips to simulate mirrored images
    \item Small random rotations (±10\%)
    \item Random zoom transformations (±10\%)
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{augmented_images.png}
    \caption{Examples of Original and Augmented Images}
\end{figure}

\subsection{Model Training}
Both models were compiled using the Adam optimizer and categorical crossentropy loss function. The training process included:

\begin{lstlisting}[language=Python, caption=Model Training]
# Compile the model
baseline_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model
history = baseline_model.fit(
    X_train, y_train_one_hot,
    batch_size=128,
    epochs=10,
    validation_data=(X_val, y_val_one_hot),
    callbacks=[checkpoint],
    verbose=1
)
\end{lstlisting}

Training configuration:
\begin{itemize}
    \item Adam optimizer with learning rate of 0.001
    \item Batch size of 128 images
    \item 10 epochs of training
    \item ModelCheckpoint callback to save the best model based on validation accuracy
\end{itemize}

For the improved model, data augmentation was applied during training:

\begin{lstlisting}[language=Python, caption=Training with Data Augmentation]
# Train the improved model with data augmentation
improved_history = improved_model.fit(
    data_augmentation(X_train, training=True), y_train_one_hot,
    batch_size=128,
    epochs=10,
    validation_data=(X_val, y_val_one_hot),
    callbacks=[checkpoint_improved],
    verbose=1
)
\end{lstlisting}

\subsection{Training Progress Visualization}
The training progress was visualized to monitor model performance and detect potential overfitting:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{training_history.png}
    \caption{Training and Validation Accuracy/Loss for Baseline Model}
\end{figure}

The training curves revealed:
\begin{itemize}
    \item Steady improvement in both training and validation accuracy
    \item A decreasing loss trend throughout training
    \item Some signs of overfitting in later epochs as the training accuracy continued to improve while validation accuracy plateaued
\end{itemize}

\subsection{Model Evaluation}
Both models were evaluated on the test dataset to assess their generalization performance:

\begin{lstlisting}[language=Python, caption=Model Evaluation]
# Evaluate the model on test data
test_loss, test_acc = baseline_model.evaluate(X_test, y_test_one_hot, verbose=1)
print(f"\nTest accuracy: {test_acc:.4f}")
print(f"Test loss: {test_loss:.4f}")

# Get predictions
y_pred = baseline_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test_one_hot, axis=1)
\end{lstlisting}

The evaluation metrics included:
\begin{itemize}
    \item Accuracy: Percentage of correctly classified images
    \item Loss: Categorical crossentropy loss on test data
    \item Confusion Matrix: Visualization of correct and incorrect classifications for each class
    \item Classification Report: Precision, recall, and F1-score for each class
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{confusion_matrix_baseline.png}
    \caption{Confusion Matrix for Baseline Model}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
airplane & 0.82 & 0.79 & 0.80 & 1000 \\
automobile & 0.92 & 0.94 & 0.93 & 1000 \\
bird & 0.74 & 0.67 & 0.70 & 1000 \\
cat & 0.65 & 0.61 & 0.63 & 1000 \\
deer & 0.75 & 0.78 & 0.77 & 1000 \\
dog & 0.71 & 0.69 & 0.70 & 1000 \\
frog & 0.84 & 0.87 & 0.86 & 1000 \\
horse & 0.83 & 0.85 & 0.84 & 1000 \\
ship & 0.89 & 0.92 & 0.90 & 1000 \\
truck & 0.88 & 0.90 & 0.89 & 1000 \\
\hline
\end{tabular}
\caption{Classification Report for Baseline Model}
\end{table}

\subsection{Visualization of Model Predictions}
To better understand model performance, sample predictions were visualized alongside their confidence scores:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{prediction_visualization.png}
    \caption{Sample Images with Predictions and Confidence Scores}
\end{figure}

The visualization revealed:
\begin{itemize}
    \item High confidence in most correct predictions
    \item Common confusion between similar classes (e.g., cat vs. dog, automobile vs. truck)
    \item Challenging cases where even a human might struggle to classify correctly
\end{itemize}

\subsection{Comparing Baseline and Improved Models}
The performance of both models was compared to assess the impact of architectural improvements:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{model_comparison.png}
    \caption{Validation Accuracy and Loss Comparison Between Models}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{Baseline Model} & \textbf{Improved Model} \\
\hline
Test Accuracy & 78.3\% & 83.7\% \\
\hline
Test Loss & 0.6985 & 0.5231 \\
\hline
\end{tabular}
\caption{Performance Comparison Between Baseline and Improved Models}
\end{table}

The comparison demonstrated:
\begin{itemize}
    \item A significant improvement in test accuracy (5.4\% absolute increase)
    \item Reduced loss in the improved model
    \item Better generalization due to regularization techniques
    \item More stable training with less overfitting
\end{itemize}

\subsection{Class-wise Performance Analysis}
To identify which classes benefited most from the architectural improvements, a class-wise accuracy comparison was performed:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{class_wise_accuracy.png}
    \caption{Per-Class Accuracy Comparison Between Models}
\end{figure}

Key findings from the class-wise analysis:
\begin{itemize}
    \item Bird, cat, and dog classes showed the largest improvements
    \item Ship and automobile classes had the smallest improvements
    \item Classes with natural variability (animals) benefited most from regularization and augmentation
\end{itemize}

\subsection{Analysis of Misclassified Examples}
To understand the models' limitations, examples that were misclassified by both models were analyzed:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{misclassified_examples.png}
    \caption{Examples Misclassified by Both Models}
\end{figure}

Common patterns in misclassifications:
\begin{itemize}
    \item Unusual poses or perspectives of objects
    \item Poor lighting conditions or low contrast
    \item Ambiguous images that could reasonably belong to multiple classes
    \item Objects partially outside the frame or obscured
\end{itemize}

\section{Challenges Faced}

Throughout the implementation, several challenges were encountered and addressed:

\begin{enumerate}
    \item \textbf{Model Complexity vs. Computational Resources:}
    \begin{itemize}
        \item Balancing model capacity with available computational resources
        \item Finding optimal batch size to fit in memory while maintaining training stability
    \end{itemize}
    
    \item \textbf{Overfitting:}
    \begin{itemize}
        \item The baseline model showing signs of overfitting to the training data
        \item Implementing appropriate regularization techniques to combat overfitting
    \end{itemize}
    
    \item \textbf{Class Confusion:}
    \begin{itemize}
        \item Difficulty distinguishing between visually similar classes (e.g., cat vs. dog)
        \item Identifying features that best separate confusing classes
    \end{itemize}
    
    \item \textbf{Hyperparameter Selection:}
    \begin{itemize}
        \item Determining optimal learning rate, number of filters, and layer configurations
        \item Balancing dropout rates to prevent both underfitting and overfitting
    \end{itemize}
    
    \item \textbf{Data Augmentation Trade-offs:}
    \begin{itemize}
        \item Ensuring augmentations maintain class identity while increasing variability
        \item Selecting appropriate augmentation parameters specific to CIFAR-10 characteristics
    \end{itemize}
\end{enumerate}

\section{Conclusion}

This lab implementation successfully developed CNN models for CIFAR-10 image classification, demonstrating the effectiveness of modern deep learning techniques. Key accomplishments include:

\begin{itemize}
    \item \textbf{Successful Data Processing:} We effectively loaded, explored, and preprocessed the CIFAR-10 dataset for neural network training.
    
    \item \textbf{Baseline Model Development:} We designed and implemented a functional CNN architecture that achieved reasonable performance on the classification task.
    
    \item \textbf{Architecture Improvements:} Through the addition of Batch Normalization, Dropout, and Data Augmentation, we created an improved model with significantly better performance.
    
    \item \textbf{Comparative Analysis:} We conducted a thorough comparison between the baseline and improved models, highlighting the benefits of modern regularization techniques.
    
    \item \textbf{Visual Interpretability:} We visualized model predictions and analyzed misclassifications to understand model strengths and limitations.
\end{itemize}

The results demonstrate that carefully designed CNN architectures with appropriate regularization can achieve strong performance on image classification tasks. The improved model, featuring Dropout, Batch Normalization, and Data Augmentation, showed a substantial performance increase over the baseline model, highlighting the importance of these techniques in modern deep learning applications.

\section{Future Work}

Several directions for future improvement and extension of this work include:

\begin{enumerate}
    \item \textbf{Advanced Architecture Exploration:} 
    \begin{itemize}
        \item Implement state-of-the-art architectures like ResNet, Inception, or EfficientNet
        \item Explore skip connections and residual blocks to improve gradient flow
    \end{itemize}
    
    \item \textbf{Hyperparameter Optimization:}
    \begin{itemize}
        \item Conduct systematic hyperparameter tuning using grid search or Bayesian optimization
        \item Experiment with learning rate schedules and adaptive optimizers
    \end{itemize}
    
    \item \textbf{Enhanced Regularization:}
    \begin{itemize}
        \item Implement label smoothing and mixup augmentation
        \item Explore self-supervised pretraining approaches
    \end{itemize}
    
    \item \textbf{Model Explainability:}
    \begin{itemize}
        \item Apply visualization techniques like Grad-CAM to understand feature importance
        \item Analyze learned filters and feature maps to interpret model decisions
    \end{itemize}
    
    \item \textbf{Transfer Learning:}
    \begin{itemize}
        \item Utilize models pretrained on larger datasets like ImageNet
        \item Fine-tune only specific layers while freezing others
    \end{itemize}
\end{enumerate}

By pursuing these directions, the image classification system could become more accurate, efficient, and interpretable, potentially reaching performance levels comparable to human classification ability on the CIFAR-10 dataset.

\section{References}
\begin{enumerate}
    \item Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. Technical Report, University of Toronto.
    
    \item He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 770-778).
    
    \item Ioffe, S., \& Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (pp. 448-456).
    
    \item Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \& Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1), 1929-1958.
    
    \item Simonyan, K., \& Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556.
    
    \item Chollet, F. (2017). Deep Learning with Python. Manning Publications Co.
    
    \item TensorFlow Documentation: \url{https://www.tensorflow.org/api_docs}
    
    \item Keras Documentation: \url{https://keras.io/api/}
\end{enumerate}

\end{document}
